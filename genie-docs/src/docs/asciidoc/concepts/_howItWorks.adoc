=== How it Works

This section is meant to provide context for how Genie can be configured with Clusters, Commands and Applications (see
<<Data Model>> for details) and then how these work together in order to run a job on a Genie node.

==== Resource Configuration

This section describes how configuration of Genie works from an administrator point of view. This isn't how to
install and configure the Genie application itself. Rather it is how to configure the various resources involved in
running a job.

===== Register Resources

All resources (clusters, commands, applications) should be registered with Genie before attempting to link them
together. Any files these resources depend on should be uploaded somewhere Genie can access them (S3, web server,
mounted disk, etc).

Tagging of the resources, particularly Clusters and Commands, is extremely important. Genie will use the tags in order
to find a cluster/command combination to run a job. You should come up with a convenient tagging scheme for your
organization. At Netflix we try to stick to a pattern for tags structures like `{tag category}:{tag value}`. For
example `type:yarn` or `data:prod`. This allows the tags to have some context so that when users look at what resources
are available they can find what to submit their jobs with so it is routed to the correct cluster/command combination.

===== Linking Resources

Once resources are registered they should be linked together. By linking we mean to represent relationships between the
resources.

====== Command Cluster Criteria

Starting in Genie V4, commands and clusters are not longer expliticly linked.

Instead, the command provides criteria describing the clusters suitable to execute it.

For example, a `spark-submit (v.2.1.1)` command may declare the following cluster criteria:
 * `type:hadoop`
 * `spark-version:2.1`

This section of the command definition says: "this command is suitable to run against hadoop clusters running version 2.1".

Implicitly, this excludes clusters that are not hadoop (say, Presto clusters), or Hadoop clusters that don't support Spark 2.1.

The command's cluster criteria are combined with the job's cluster criteria to further narrow the set of viable clusters.

====== Applications for a Command

Linking application(s) to commands means that a command has a dependency on said application(s). The order of the
applications added is important because Genie will setup the applications in that order. Meaning if one application
depends on another (e.g. Spark depends on Hadoop on classpath for YARN mode) Hadoop should be ordered first. All
applications must successfully be installed before Genie will start running the job.

==== Job Submission

The system admin has everything registered and linked together. Things could change but that's mostly
transparent to end users, who just want to run jobs. How does that work? This section walks through what
happens at a high level when a job is submitted.

===== Cluster and command matching

In order to submit a job request there is some work a user will have to do up front. What kind of job are they running?
What cluster do they want to run on? What command do they want to use? Do they care about certain details like version
or just want the defaults? Once they determine the answers to the questions they can decide how they want to tag their
job request for the `clusterCriterias` and `commandCriteria` fields.

General rule of thumb for these fields is to use the lowest common denominator of tags to accomplish what a user
requires. This will allow the most flexibility for the job to be moved to different clusters or commands as need be.
For example if they want to run a Spark job and don't really care about version it is better to just say
"type:sparksubmit" (assuming this is tagging structure at your organization) only instead of that *and* "ver:2.0.0".
This way when versions 2.0.1 or 2.1.0 become available, the job moves along with the new default. Obviously if they do
care about version they should set it or any other specific tag.

The `clusterCriterias` field is an array of `ClusterCriteria` objects. This is done to provide a fallback mechanism.
If no match is found for the first `ClusterCriteria` and `commandCriteria` combination it will move onto the second
and so on until all options are exhausted. This is handy if it is desirable to run a job on some cluster that is only
up some of the time but other times it isn't and its fine to run it on some other cluster that is always available.

NOTE: Only clusters with status `UP` and commands with status `ACTIVE` will be considered during the selection process
all others are ignored.

====== Cluster matching example

Say the following 3 clusters exists tagged as follows:

PrestoTestCluster:
. sched:test
. type:presto
. ver:0.149

HadoopProdCluster:
. sched:sla
. type:yarn
. ver:2.7.0
. ver:2.7

HadoopTestCluster:
. sched:test
. type:yarn
. ver:2.7.1
. ver:2.7


|===
| Criteria | Match | Reason

| `type:yarn, ver:2.7, sched:sla`
| HadoopProdCluster
| HadoopProdCluster satisfies all criteria

| `type:yarn, ver:2.7`
| HadoopProdCluster or HadoopTestCluster
| Two clusters satisfy the criteria, a choice behavior is unspecified

| `type:yarn, ver:2.7.1`
| HadoopTestCluster
| HadoopTestCluster satisfies all criteria

| `type:presto, ver:0.150`
| -
| No cluster matches all criteria

| `[type:presto, ver:0.150], [type:presto]`
| PrestoTestCluster
| The first criteria does not match any cluster, so fallback happens to the second, less restrictive criteria
("any presto cluster").

|===

===== User Submits a Job Request

There are other things a user needs to consider when submitting a job. All dependencies which aren't sent as attachments
must already be uploaded somewhere Genie can access them. Somewhere like S3, web server, shared disk, etc.

Users should familiarize themselves with whatever the `executable` for their desired command includes. It's possible
the system admin has setup some default parameters they should know are there so as to avoid duplication or unexpected
behavior. Also they should make sure they know all the environment variables that may be available to them as part of
the setup process of all the cluster, command and application setup processes.

===== Genie Receives the Job Request

When Genie receives the job request it does a few things immediately:

. If the job request doesn't have an id it creates a GUID for the job
. It saves the job request to the database so it is recorded
.. If the ID is in use a 409 will be returned to the user saying there is a conflict
. It creates job and job execution records in data base for consistency
. It saves any attachments in a temporary location

Next Genie will attempt to find a cluster and command matching the requested tag combinations. If none is found it will
send a failure back to the user and mark the job failed in the database.

If a combination is found Genie will then attempt to determine if the node can run the job. By this it means it will
check the amount of client memory the job requires against the available memory in the Genie allocation. If there is
enough the job will be accepted and will be run on this node and the jobs memory is subtracted from the available pool.
If not it will be rejected with a 503 error message and user should retry later.

The amount of memory used by a job is not strictly enforced or even monitored. Such size is determined as follows:

. Account for the amount requested in the job request (which must be below an admin-defined threshold)
. If not provided in the request, use the number provided by the admins for the given command
. If not provided in the command, use a global default set by the admins

Successful job submission results in a 202 message to the user stating it's accepted and will be processed
asynchronously by the system.

===== Genie Performs Job Setup

Once a job has been accepted to run on a Genie node, a workflow is executed in order to setup the job working directory
and launch the job. Some minor steps left out for brevity.

. Job is marked in `INIT` state in the database
. A job directory is created under the admin configured jobs directory with a name of the job id
. A run script file is created with the name `run` under the job working directory
.. Currently this is a bash script
. Kill handlers are added to the run script
. Directories for Genie logs, application files, command files, cluster files are created under the job working
directory
. Default environment variables are added to the run script to export their values
. Cluster configuration files are downloaded and stored in the job work directory
. Cluster related variables are written into the run script
. Application configuration and dependency files are downloaded and stored in the job directory if any applications are
needed
. Application related variables are written into the run script
. Cluster configuration and dependency files are downloaded and stored in the job directory
. Command configuration and dependency files are downloaded and stored in the job directory
. Command related variables are written into the run script
. All job dependency files (including configurations, dependencies, attachments) are downloaded into the job working directory
. Job related variables are written into the run script

===== Genie Launches and Monitors the Job Execution

Assuming no errors occurred during the setup, the job is launched.

. Job `run` script is executed in a forked process.
. Script `pid` stored in database `job_executions` table and job marked as `RUNNING` in database
. Monitoring process created for pid

Once the job is running Genie will poll the PID periodically waiting for it to no longer be used.

NOTE: Assumption made as to the amount of process churn on the Genie node. We're aware PID's can be reused but
reasonably this shouldn't happen within the poll period given the amount of available PID to the processes a typical
Genie node will run.

Once the pid no longer exists Genie checks the done file for the exit code. It marks the job succeeded, failed or
killed depending on that code.

===== Genie Performs Job Clean-Up

To save disk space Genie will delete application, cluster and command dependencies from the job working directory after a job is completed.
This can be disabled by an admin. If the job is marked as it should be archived the working directory will be zipped up
and stored in the default archive location as `{jobId}.tar.gz`.

==== User Behavior

Users can check on the status of their job using the `status` API and get the output using the output APIs. See the
https://netflix.github.io/genie/docs/{revnumber}/rest/[REST Documentation] for specifics on how to do that.

==== Wrap Up

This section should have helped you understand how Genie works at a high level from configuration all the way to user
job submission and monitoring. The design of Genie is intended to make this process repeatable and reliable for all
users while not hiding any of the details of what is executed at job runtime.
