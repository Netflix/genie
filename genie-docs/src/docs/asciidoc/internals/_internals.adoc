== Internals

This section provides an overview of Genie subsystems and components.

=== Leader election

Genie uses Zookeeper to elect an instance to be leader.
An alternative leader election mechanism can be plugged in instead of Zookeeper.

==== Leader tasks

The Genie instance elected leader of a given cluster performs different tasks:

 * Trimming old job records, disabling of unused resources
 * Mark jobs whose agent is AWOL as failed
 * Collect and publish user metrics about running jobs

=== Execution stages

Every job is executed by a dedicated agent.
This is true for both API-submitted jobs and CLI-initiated jobs.

An agent executing a job goes through the following stages:

 * Initialize - Gather metadata about the agent itself and its environment
 * Handshake - Connect to the Genie cluster and provide agent metadata
 ** This allows the cluster to turn away an agent that is too old, or running from an unsupported environment
 * Configure agent - Download additional properties from the server
 ** This allows the server to provide configuration, for example set a different network timeout, or change log levels
 * Configure execution - Configure the rest of the execution stages depending on the command-line arguments
 ** For example, executing a pre-resolved job submitted via API vs. compose a job request for a CLI job
 * Reserve job - If executing a CLI job, save the job request obtained from command-line arguments
 * Obtain job specification
 ** For a CLI job, the agent asks the server to resolve the job request into a specification
 ** For an API job, the agent downloads the pre-resolved job specification
 * Create job directory
 * Relocate the agent log - From a temporary location to the job directory, so it's accessible during execution and archived afterwards
 * Claim job - Atomically claim a job so no other agents will be able to
 * Start heartbeat - The agent needs to constantly tell the Genie cluster that it is still active
 * Register for kill notifications - Allowing a user to kill a job via API
 * Start files service - Allowing a user to download agent logs and outputs during execution
 * Update job status to INIT
 * Create the job `run` script inside the job directory
 * Download job dependencies, attachments, configurations, setup scripts inside the job directory
 * Launch and monitor the job `run` script
 * Update job status to RUNNING
 * Monitor the job process until it completes
 * Determine the job final status (`SUCCEEDED`, `FAILED`, `KILLED`)
 * Update the final job status
 * Unregister for kill notifications
 * Archive the job logs and output to long-term storage
 * Stop heart-beating
 * Stop serving files (allowing some time for in-progress transfers to complete)
 * Clean the job directory (removing redundant files such as libraries and binaries, to avoid wasting space on disk)
 * Shutdown
 ** The agent exit code reflects the job final status

For a successful job, these steps are executed linearly.

However, when something goes wrong, stages may be skipped.
For example, if an agent is rejected by the cluster during the Handshake stage, execution is aborted, and most of the following stages are skipped.

Some stages may fail due to transient issues, in which case they are retried (for example, Handshake may fail due to a broken connection, but a retry may succeed).
If a retryable stage runs out of attempts, then execution is aborted, which may skip following stages as appropriate.

=== The `run` script

Every job consists of a `bash` script that the agent composes, forks, and monitors.

This separation allows treating every job the same way, whether they are Spark or Notebook kernels, or anything else.

The structure of a `run` script is as follows:

 * `bash` options
 * Signal traps, to ensure clean shutdown and handing of `ctrl-c` for CLI jobs
 * Declaration of environment variables
 ** These can be used by setup scripts and by the command itself
 * Sourcing of setup script (if the selected entities provide them)
 ** For example, the setup for `Hadoop` may unpack the spark libraries, or set additional environment variables
 ** The order of setup is: cluster, applications (in order defined by command), command
 ** Setup output is piped to a dedicated log file
 * Dump of environment variables
 ** Sensitive environment variables can be filtered, or the dumping can be disabled completely
 * Launch the assembled job command
 * Wait for command termination

=== Agent heartbeat and routing

Each agent that successfully claimed a job maintains a gRPC connection to a server in the cluster and periodically sends heartbeats.

It does not matter which server instance an agent is connected to.
If the gRPC endpoint is fronted by a TCP load-balancer, upon disconnection the agent will reconnect to a different instance.

The server instances use heartbeat to know which clients are connected to each instance.
This information is stored in the agent routing table.
The routing table is used by Genie server instances to dispatch a kill request to the instance where the agent is currently connected.
That instance then forwards the request to the agent.

=== Agent file streaming

Each agent executing a job opens multiple streaming channels on top of a single connection to a server.
These channels can be used to pull a file from the agent and serve it via REST API

=== Agent logging

Log messages emitted by a genie agent are routed to `agent.log`.
This file is created at a temporary location when the agent starts, and relocated to the job folder once the latter exists.

Some messages (such as fatal errors and high level execution progress updates) are sent to a special logger that displays them in the console.
These messages are appended to `stderr` to avoid polluting the job command output (sent to `stdout`) such as query outputs.

=== Interactive mode

Agent execute by default in non-interactive mode:

 * Job standard error is appended to a `stderr` file
 * Job standard output is appended to a `stdout` file
 * Standard input is ignored

Hence, the agent and the child job process do not emit anything to console.

Commands that launch REPL interfaces (e.g., `spark-shell`), should be launched via CLI with the `--interactive` flag.

* Job standard error and output go directly to the shell
* Standard input is received by the job process directly

All jobs launched by the server (as a result of an API job submission) execute in non-interactive mode.

=== Persistence

Out of the box, Genie supports MySQL and Postgres persistence via JPA and Hibernate.

H2 in-memory database is also supported for integration tests.

Other kinds of persistence can be plugged in either via JPA or by re-implementing the persistence interface with a different technology.

=== Plug-in Selectors

Genie provides a mechanism to load and invoke Groovy scripts, and even reload the script if it changes at runtime.

These hooks are provided in order to plug-in custom logic:

==== Command selector script

A job command criteria may be broad enough to match multiple commands (for example, a Spark job may match 10 different Spark commands, one for each Spark version).

The command selector allows custom plug-in logic to choose the most appropriate.

For example, there may be a 'stable' version which should be picked as default.
Or the job may be a good candidate to "canary" the newest and unstable version.

==== Cluster selector script

Clusters that are eligible to run a given job are filtered down by the command's cluster criteria combined with the job's cluster criteria.
But the final match set may still contain more than one viable cluster.

In this kind of situation, the selector script can make advanced decision that go beyond Genie's scope.

For example, the script could poll the clusters and see which one is the least loaded.
Or route the job based on other attributes or metadata.

==== Agent launcher selector script

Genie supports multiple agent launcher.
For example, a launcher that starts the job on bare metal on in the same host, a launcher that starts the job inside a Docker container, a launcher that delegates the job to something like Kubernetes.

Like in previous examples, making this runtime decisions is beyond the scope of Genie itself.

Therefore a hook is provided to plug-in a Groovy selector script that can make this decision based on job metadata and pick one of the available launchers.

=== Job State Change Notifications

Genie emits internal application events whenever a job changes state.

Genie includes 2 built-in components that consume theese events and publish notifications to Amazon SNS.

=== User limits

Genie can cap the maximum number of jobs that a given user is allowed to run.

The global default can be adjusted, and overrides for individual users can be set higher or lower.

=== Job attachments

Genie allows API job requests to submit one or more attachments (by submitting a multi-part POST).
These files are placed in the job directory before launch.

Example usage include Presto scripts, and Spark application jars, and additional property files.

=== Job archival

After a job is completed, successfully or not, job file outputs and logs can be archived for later access.

The set of files that get archived is configurable. Or it can be disabled completely.
